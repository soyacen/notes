# 健康检查
健康检查是指容器运行过程中，根据用户需要，定时检查容器健康状况。若不配置健康检查，如果容器内应用程序异常，Pod将无法感知，也不会自动重启去恢复。最终导致虽然Pod状态显示正常，但Pod中的应用程序异常的情况。

Kubernetes提供了三种健康检查的探针：

* 存活探针：livenessProbe，用于检测容器是否正常，类似于我们执行ps命令检查进程是否存在。如果容器的存活检查失败，集群会对该容器执行重启操作；若容器的存活检查成功则不执行任何操作。
* 就绪探针：readinessProbe，用于检查用户业务是否就绪，如果未就绪，则不转发流量到当前实例。一些程序的启动时间可能很长，比如要加载磁盘数据或者要依赖外部的某个模块启动完成才能提供服务。这时候程序进程在，但是并不能对外提供服务。这种场景下该检查方式就非常有用。如果容器的就绪检查失败，集群会屏蔽请求访问该容器；若检查成功，则会开放对该容器的访问。
* 启动探针：startupProbe，用于探测应用程序容器什么时候启动了。 如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查， 确保这些存活、就绪探针不会影响应用程序的启动。 这可以用于对启动慢的容器进行存活性检测，避免它们在启动运行之前就被杀掉。

# 存活探针（Liveness Probe）
Kubernetes提供了自愈的能力，具体就是能感知到容器崩溃，然后能够重启这个容器。

但是有时候例如Java程序内存泄漏了，程序无法正常工作，但是JVM进程却是一直运行的，对于这种应用本身业务出了问题的情况，Kubernetes提供了Liveness Probe机制，通过检测容器响应是否正常来决定是否重启，这是一种很好的健康检查机制。

毫无疑问，每个Pod最好都定义Liveness Probe，否则Kubernetes无法感知Pod是否正常运行。

## Kubernetes支持如下三种探测机制。

* HTTP GET：向容器发送HTTP GET请求，如果Probe收到2xx或3xx，说明容器是健康的。
* TCP Socket：尝试与容器指定端口建立TCP连接，如果连接成功建立，说明容器是健康的。
* Exec：Probe执行容器中的命令并检查命令退出的状态码，如果状态码为0则说明容器是健康的。

## HTTP GET
HTTP GET方式是最常见的探测方法，其具体机制是向容器发送HTTP GET请求，如果Probe收到2xx或3xx，说明容器是健康的，定义方法如下所示。
```
apiVersion: v1
kind: Pod
metadata:
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: nginx:alpine
    livenessProbe:           # liveness probe
      httpGet:               # HTTP GET定义
        path: /
        port: 80
  imagePullSecrets: 
  - name: default-secret
```

创建这个Pod:
```
$ kubectl create -f liveness-http.yaml
pod/liveness-http created
```
如上，这个Probe往容器的80端口发送HTTP GET请求，如果请求不成功，Kubernetes会重启容器。

查看Pod详情。
```
$ kubectl describe po liveness-http
Name:               liveness-http
......
Containers:
  liveness:
    ......
    State:          Running
      Started:      Mon, 03 Aug 2020 03:08:55 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vssmw (ro)
......
```

可以看到Pod当前状态是Running，Restart Count为0，说明没有重启。如果Restart Count不为0，则说明已经重启。

## TCP Socket
TCP Socket尝试与容器指定端口建立TCP连接，如果连接成功建立，说明容器是健康的，定义方法如下所示。
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-tcp
spec:
  containers:
  - name: liveness
    image: nginx:alpine
    livenessProbe:           # liveness probe
      tcpSocket:
        port: 80
  imagePullSecrets: 
  - name: default-secret
```

## Exec
Exec即执行具体命令，具体机制是Probe执行容器中的命令并检查命令退出的状态码，如果状态码为0则说明健康，定义方法如下所示。
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: nginx:alpine
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:           # liveness probe
      exec:                  # Exec定义
        command:
        - cat
        - /tmp/healthy
  imagePullSecrets: 
  - name: default-secret
```
上面定义在容器中执行cat /tmp/healthy命令，如果成功执行并返回0，则说明容器是健康的。上面定义中，30秒后命令会删除/tmp/healthy，这会导致Liveness Probe判定Pod处于不健康状态，然后会重启容器。

## Liveness Probe高级配置
上面liveness-http的describe命令回显中有如下行。
```
Liveness: http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
```
这一行表示Liveness Probe的具体参数配置，其含义如下：

* delay：延迟，delay=0s，表示在容器启动后立即开始探测，没有延迟时间
* timeout：超时，timeout=1s，表示容器必须在1s内进行响应，否则这次探测记作失败
* period：周期，period=10s，表示每10s探测一次容器
* success：成功，#success=1，表示连续1次成功后记作成功
* failure：失败，#failure=3，表示连续3次失败后会重启容器
  
以上存活探针表示：容器启动后立即进行探测，如果1s内容器没有给出回应则记作探测失败。每次间隔10s进行一次探测，在探测连续失败3次后重启容器。

这些是创建时默认设置的，您也可以手动配置，如下所示。
```
apiVersion: v1
kind: Pod
metadata:
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: nginx:alpine
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10    # 容器启动后多久开始探测
      timeoutSeconds: 2          # 表示容器必须在2s内做出相应反馈给probe，否则视为探测失败
      periodSeconds: 30          # 探测周期，每30s探测一次
      successThreshold: 1        # 连续探测1次成功表示成功
      failureThreshold: 3        # 连续探测3次失败表示失败
```
initialDelaySeconds一般要设置大于0，这是由于很多情况下容器虽然启动成功，但应用就绪也需要一定的时间，需要等就绪时间之后才能返回成功，否则就会导致probe经常失败。

另外failureThreshold可以设置多次循环探测，这样在实际应用中健康检查的程序就不需要多次循环，这一点在开发应用时需要注意。

## 配置有效的Liveness Probe
### Liveness Probe应该检查什么
一个好的Liveness Probe应该检查应用内部所有关键部分是否健康，并使用一个专有的URL访问，例如/health，当访问/health 时执行这个功能，然后返回对应结果。这里要注意不能做鉴权，不然probe就会一直失败导致陷入重启的死循环。

另外检查只能限制在应用内部，不能检查依赖外部的部分，例如当前端web server不能连接数据库时，这个就不能看成web server不健康。

### Liveness Probe必须轻量
Liveness Probe不能占用过多的资源，且不能占用过长的时间，否则所有资源都在做健康检查，这就没有意义了。例如Java应用，就最好用HTTP GET方式，如果用Exec方式，JVM启动就占用了非常多的资源。

# 就绪探针（Readiness Probe）
一个新Pod创建后，Service就能立即选择到它，并会把请求转发给Pod，那问题就来了，通常一个Pod启动是需要时间的，如果Pod还没准备好（可能需要时间来加载配置或数据，或者可能需要执行一个预热程序之类），这时把请求转给Pod的话，Pod也无法处理，造成请求失败。

Kubernetes解决这个问题的方法就是给Pod加一个业务就绪探针Readiness Probe，当检测到Pod就绪后才允许Service将请求转给Pod。

Readiness Probe同样是周期性的检测Pod，然后根据响应来判断Pod是否就绪，与存活探针（Liveness Probe）相同，就绪探针也支持如下三种类型。

* Exec：Probe执行容器中的命令并检查命令退出的状态码，如果状态码为0则说明已经就绪。
* HTTP GET：往容器的IP:Port发送HTTP GET请求，如果Probe收到2xx或3xx，说明已经就绪。
* TCP Socket：尝试与容器建立TCP连接，如果能建立连接说明已经就绪。
  
## Readiness Probe的工作原理
通过Endpoints就可以实现Readiness Probe的效果，当Pod还未就绪时，将Pod的IP:Port从Endpoints中删除，Pod就绪后再加入到Endpoints中，如下图所示。

![图1 Readiness Probe的实现原理](../image/zh-cn_image_0259536449.png)


## Exec
Exec方式与HTTP GET方式一致，如下所示，这个探针执行ls /ready命令，如果这个文件存在，则返回0，说明Pod就绪了，否则返回其他状态码。
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:alpine
        name: container-0
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        readinessProbe:      # Readiness Probe
          exec:              # 定义 ls /ready 命令
            command:
            - ls
            - /ready
      imagePullSecrets:
      - name: default-secret
```
将上面Deployment的定义保存到deploy-read.yaml文件中，删除之前创建的Deployment，用deploy-read.yaml创建这个Deployment。
```
# kubectl delete deploy nginx
deployment.apps "nginx" deleted

# kubectl create -f deploy-read.yaml
deployment.apps/nginx created
```
这里由于nginx镜像不包含/ready这个文件，所以在创建完成后容器不在Ready状态，如下所示，注意READY这一列的值为0/1，表示容器没有Ready。
```
# kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-7955fd7786-686hp   0/1       Running   0          7s
nginx-7955fd7786-9tgwq   0/1       Running   0          7s
nginx-7955fd7786-bqsbj   0/1       Running   0          7s
```
创建Service。
```
apiVersion: v1
kind: Service
metadata:
  name: nginx        
spec:
  selector:          
    app: nginx
  ports:
  - name: service0
    targetPort: 80   
    port: 8080       
    protocol: TCP    
  type: ClusterIP
```
查看Service，发现Endpoints一行的值为空，表示没有Endpoints。
```
$ kubectl describe svc nginx
Name:              nginx
......
Endpoints:         
......
```
如果此时给容器中创建一个/ready的文件，让Readiness Probe成功，则容器会处于Ready状态。再查看Pod和Endpoints，发现创建了/ready文件的容器已经Ready，Endpoints也已经添加。
```
# kubectl exec nginx-7955fd7786-686hp -- touch /ready

# kubectl get po -o wide
NAME                     READY     STATUS    RESTARTS   AGE       IP
nginx-7955fd7786-686hp   1/1       Running   0          10m       192.168.93.169 
nginx-7955fd7786-9tgwq   0/1       Running   0          10m       192.168.166.130
nginx-7955fd7786-bqsbj   0/1       Running   0          10m       192.168.252.160

# kubectl get endpoints
NAME       ENDPOINTS           AGE
nginx      192.168.93.169:80   14d
```
## HTTP GET
Readiness Probe的配置与存活探针（livness probe）一样，都是在Pod Template的containers里面，如下所示，这个Readiness Probe向Pod发送HTTP请求，当Probe收到2xx或3xx返回时，说明Pod已经就绪。
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:alpine
        name: container-0
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        readinessProbe:           # readinessProbe
          httpGet:                # HTTP GET定义
            path: /read
            port: 80
      imagePullSecrets:
      - name: default-secret
```

## TCP Socket
同样，TCP Socket类型的探针如下所示。
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:alpine
        name: container-0
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        readinessProbe:             # readinessProbe
          tcpSocket:                # TCP Socket定义
            port: 80
      imagePullSecrets:
      - name: default-secret
```

## Readiness Probe高级配置
与Liveness Probe相同，Readiness Probe也有同样的高级配置选项，上面nginx Pod的describe命令回显有中有如下行。
```
Readiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3
```
这一行表示Readiness Probe的具体参数配置，其含义如下：

* delay=0s 表示容器启动后立即开始探测，没有延迟时间
* timeout=1s 表示容器必须在1s内做出相应反馈给probe，否则视为探测失败
* period=10s 表示每10s探测一次
* #success=1 探测连续1次成功表示成功
* #failure=3 探测连续3次失败表示失败
  
这些是创建时默认设置的，您也可以手动配置，如下所示。
```
        readinessProbe:      # Readiness Probe
          exec:              # 定义 ls /readiness/ready 命令
            command:
            - ls
            - /readiness/ready
          initialDelaySeconds: 10    # 容器启动后多久开始探测
          timeoutSeconds: 2          # 表示容器必须在2s内做出相应反馈给probe，否则视为探测失败
          periodSeconds: 30          # 探测周期，每30s探测一次
          successThreshold: 1        # 连续探测1次成功表示成功
          failureThreshold: 3        # 连续探测3次失败表示失败
```

# 总结
设置三种探针，监测应用的健康状态，保证集群的高可用
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: nginx:alpine
    args:
    - /server
    livenessProbe:
      httpGet:
        path: /healthz
        port: 80
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
    readinessProbe:
      exec:
        command:
          - cat
          - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
    startupProbe:
      httpGet:
        path: /healthz
        port: 80
      failureThreshold: 30
      periodSeconds: 10
```

# 启动探针 （Startup Probe）
当Container还在启动的过程中，还没完成启动，此时如果livenessProbe开始工作，那么就可能会导致livenessProbe的执行结果失败，进而导致重启Container。最终有可能导致这个Container进入无限循环的不断重启的情况发生。

startupProbe用来解决这个场景下的问题。

需要更长的时间来启动，

## startupProbe的作用
startupProbe在Container启动时就开始工作，它确保该Container一定可以启动成功。

如果startupProbe的执行结果失败，那么它就会重启Container，直到Container启动成功。

如果startupProbe的执行结果成功，那么它认为Container启动成功，接下来才可以开始执行livenessProbe，或者是Container可以开始接收应用请求了。这样就避免了Container还没启动完成，应用请求或者是livenessProbe的请求就发送过来，导致得不到预期的响应结果的情况发生。

## startupProbe的分类和参数
跟livenessProbe和readinessProbe一样分为3类：

* httpGet请求；
* TCPSocket请求；
* exec命令；
在Kubernetes 1.23版本开始，还支持gRPC类型的probe。

关于每一种类型和之前都差不多。

## startupProbe举例
### 1 启动正常的exec类型的startupProbe
```
[root@master-node ~]# cat startup-probe-exec-succeed-demo.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: startup-probe-exec-succeed-demo
spec:
  containers:
  - name: startup-probe-exec-succeed-demo
    image: busybox:latest
    args:
    - /bin/sh
    - -c
    - sleep 300
    startupProbe:
      exec:
        command:
        - cat
        - /etc/hosts
      periodSeconds: 10
      failureThreshold: 10
[root@master-node ~]# 
```
启动pod，并查看：
```
[root@master-node ~]# kubectl apply -f startup-probe-exec-succeed-demo.yaml 
pod/startup-probe-exec-succeed-demo created
[root@master-node ~]# kubectl describe pod startup-probe-exec-succeed-demo 
Name:         startup-probe-exec-succeed-demo
Namespace:    default
Priority:     0
Node:         node-1/172.16.11.148
Start Time:   Sun, 29 May 2022 08:34:53 +0800
...
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  9s    default-scheduler  Successfully assigned default/startup-probe-exec-succeed-demo to node-1
  Normal  Pulling    8s    kubelet            Pulling image "busybox:latest"
  Normal  Pulled     4s    kubelet            Successfully pulled image "busybox:latest" in 3.715726337s
  Normal  Created    4s    kubelet            Created container startup-probe-exec-succeed-demo
  Normal  Started    4s    kubelet            Started container startup-probe-exec-succeed-demo
[root@master-node ~]# 
```
该Container里的startupProbe是exec类型的，cat /etc/hosts 命令结果是否为0 ? 作为判断条件。

### 2 启动失败的exec类型的startupProbe
```
[root@master-node ~]# cat startup-probe-exec-failure-demo.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: startup-probe-exec-failure-demo
spec:
  containers:
  - name: startup-probe-exec-failure-demo
    image: busybox:latest
    args:
    - /bin/sh
    - -c
    - sleep 300
    startupProbe:
      exec:
        command:
        - cat
        - /etc/foobar
      periodSeconds: 5
      failureThreshold: 3
[root@master-node ~]# 
```
Container里压根儿就不存在/etc/foobar文件，startupProbe必然失败，那么Container就会被重启。下一次startupProbe又失败，又重启，直到该错误解决为止。
```
[root@master-node ~]# kubectl apply -f startup-probe-exec-failure-demo.yaml 
pod/startup-probe-exec-failure-demo created
[root@master-node ~]# 
```
在另外一个terminal上执行：kubectl get events -w ;会看到类似下述错误：
```
[root@master-node ~]# kubectl get events -w
....
0s          Normal    Scheduled              pod/startup-probe-exec-failure-demo   Successfully assigned default/startup-probe-exec-failure-demo to node-2
0s          Normal    Pulling                pod/startup-probe-exec-failure-demo   Pulling image "busybox:latest"
0s          Normal    Pulled                 pod/startup-probe-exec-failure-demo   Successfully pulled image "busybox:latest" in 5.199006765s
0s          Normal    Created                pod/startup-probe-exec-failure-demo   Created container startup-probe-exec-failure-demo
0s          Normal    Started                pod/startup-probe-exec-failure-demo   Started container startup-probe-exec-failure-demo
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Normal    Killing                pod/startup-probe-exec-failure-demo   Container startup-probe-exec-failure-demo failed startup probe, will be restarted
0s          Normal    Pulling                pod/startup-probe-exec-failure-demo   Pulling image "busybox:latest"
0s          Normal    Pulled                 pod/startup-probe-exec-failure-demo   Successfully pulled image "busybox:latest" in 3.68318295s
0s          Normal    Created                pod/startup-probe-exec-failure-demo   Created container startup-probe-exec-failure-demo
0s          Normal    Started                pod/startup-probe-exec-failure-demo   Started container startup-probe-exec-failure-demo
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Normal    Killing                pod/startup-probe-exec-failure-demo   Container startup-probe-exec-failure-demo failed startup probe, will be restarted
0s          Normal    Pulling                pod/startup-probe-exec-failure-demo   Pulling image "busybox:latest"
0s          Normal    Pulled                 pod/startup-probe-exec-failure-demo   Successfully pulled image "busybox:latest" in 3.667381851s
0s          Normal    Created                pod/startup-probe-exec-failure-demo   Created container startup-probe-exec-failure-demo
0s          Normal    Started                pod/startup-probe-exec-failure-demo   Started container startup-probe-exec-failure-demo
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Warning   Unhealthy              pod/startup-probe-exec-failure-demo   Startup probe failed: cat: can't open '/etc/foobar': No such file or directory
0s          Normal    Killing                pod/startup-probe-exec-failure-demo   Container startup-probe-exec-failure-demo failed startup probe, will be restarted
```
### 3 启动正常的httpGet类型的startupProbe
```
[root@master-node ~]# cat startup-probe-httget-succeed-demo.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: startup-probe-httpget-succeed-demo
spec:
  containers:
  - name: startup-probe-httpget-succeed-demo
    image: nginx:latest
    startupProbe:
      httpGet:
        path: /
        port: 80
      periodSeconds: 5
      failureThreshold: 3
[root@master-node ~]# 
```
启动，并监控pod和events：
```
[root@master-node ~]# kubectl apply -f startup-probe-httget-succeed-demo.yaml 
pod/startup-probe-httpget-succeed-demo created
[root@master-node ~]# 
​
[root@master-node ~]# kubectl get pods -w
....
startup-probe-httpget-succeed-demo   0/1     Pending            0                0s
startup-probe-httpget-succeed-demo   0/1     Pending            0                0s
startup-probe-httpget-succeed-demo   0/1     ContainerCreating   0                0s
startup-probe-httpget-succeed-demo   0/1     Running             0                19s
startup-probe-httpget-succeed-demo   0/1     Running             0                20s
startup-probe-httpget-succeed-demo   1/1     Running             0                21s
​
​
[root@master-node ~]# kubectl get events -w
LAST SEEN   TYPE      REASON                 OBJECT                                MESSAGE
0s          Normal    Scheduled              pod/startup-probe-httpget-succeed-demo   Successfully assigned default/startup-probe-httpget-succeed-demo to node-2
0s          Normal    Pulling                pod/startup-probe-httpget-succeed-demo   Pulling image "nginx:latest"
0s          Normal    Pulled                 pod/startup-probe-httpget-succeed-demo   Successfully pulled image "nginx:latest" in 17.338565388s
0s          Normal    Created                pod/startup-probe-httpget-succeed-demo   Created container startup-probe-httpget-succeed-demo
0s          Normal    Started                pod/startup-probe-httpget-succeed-demo   Started container startup-probe-httpget-succeed-demo
```
### 4 启动失败的httpGet类型的startupProbe
```
[root@master-node ~]# cat startup-probe-httget-failure-demo.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: startup-probe-httpget-failure-demo
spec:
  containers:
  - name: startup-probe-httpget-failure-demo
    image: nginx:latest
    startupProbe:
      httpGet:
        path: /
        port: 80
        scheme: HTTPS
        httpHeaders: 
        - name: X-Client-Identity
          value: Kubernetes-Startup-Probe
[root@master-node ~]# 
```
在向NGINX的HTTP请求头了加入了信息。我们通过HTTPS协议，默认情况下NGINX并没开启支持HTTPS，所以startupProbe会报错！
```
[root@master-node ~]# kubectl apply -f startup-probe-httget-failure-demo.yaml 
pod/startup-probe-httpget-failure-demo created
[root@master-node ~]# 
...
[root@master-node ~]# kubectl get events -w
LAST SEEN   TYPE      REASON                 OBJECT                                   MESSAGE
...
0s          Normal    Scheduled              pod/startup-probe-httpget-failure-demo   Successfully assigned default/startup-probe-httpget-failure-demo to node-1
0s          Normal    Pulling                pod/startup-probe-httpget-failure-demo   Pulling image "nginx:latest"
0s          Normal    Pulled                 pod/startup-probe-httpget-failure-demo   Successfully pulled image "nginx:latest" in 18.746863622s
0s          Normal    Created                pod/startup-probe-httpget-failure-demo   Created container startup-probe-httpget-failure-demo
0s          Normal    Started                pod/startup-probe-httpget-failure-demo   Started container startup-probe-httpget-failure-demo
0s          Warning   Unhealthy              pod/startup-probe-httpget-failure-demo   Startup probe failed: Get "https://10.244.1.58:80/": http: server gave HTTP response to HTTPS client
0s          Warning   Unhealthy              pod/startup-probe-httpget-failure-demo   Startup probe failed: Get "https://10.244.1.58:80/": http: server gave HTTP response to HTTPS client
0s          Warning   EvictionThresholdMet   node/master-node                         Attempting to reclaim ephemeral-storage
0s          Warning   Unhealthy              pod/startup-probe-httpget-failure-demo   Startup probe failed: Get "https://10.244.1.58:80/": http: server gave HTTP response to HTTPS client
0s          Normal    Killing                pod/startup-probe-httpget-failure-demo   Container startup-probe-httpget-failure-demo failed startup probe, will be restarted
0s          Normal    Pulling                pod/startup-probe-httpget-failure-demo   Pulling image "nginx:latest"
0s          Normal    Pulled                 pod/startup-probe-httpget-failure-demo   Successfully pulled image "nginx:latest" in 3.744738269s
0s          Normal    Created                pod/startup-probe-httpget-failure-demo   Created container startup-probe-httpget-failure-demo
0s          Normal    Started                pod/startup-probe-httpget-failure-demo   Started container startup-probe-httpget-failure-demo
0s          Warning   Unhealthy              pod/startup-probe-httpget-failure-demo   Startup probe failed: Get "https://10.244.1.58:80/": http: server gave HTTP response to HTTPS client
```
### 5 启动正常的tcpSocket类型的startupProbe
```
[root@master-node ~]# cat startup-probe-tcpsocket-succeed-demo.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: startup-probe-tcpsocket-succeed-demo
spec:
  containers:
  - name: startup-probe-tcpsocket-succeed-demo
    image: nginx:latest
    startupProbe:
      tcpSocket:
        port: 80
      periodSeconds: 5
      failureThreshold: 3
[root@master-node ~]# kubectl apply -f startup-probe-tcpsocket-succeed-demo.yaml 
pod/startup-probe-tcpsocket-succeed-demo created
[root@master-node ~]# 
​
[root@master-node ~]# kubectl get events -w
LAST SEEN   TYPE      REASON                 OBJECT                                     MESSAGE
...
19s         Normal    Scheduled              pod/startup-probe-tcpsocket-succeed-demo   Successfully assigned default/startup-probe-tcpsocket-succeed-demo to node-2
18s         Normal    Pulling                pod/startup-probe-tcpsocket-succeed-demo   Pulling image "nginx:latest"
14s         Normal    Pulled                 pod/startup-probe-tcpsocket-succeed-demo   Successfully pulled image "nginx:latest" in 3.686109984s
14s         Normal    Created                pod/startup-probe-tcpsocket-succeed-demo   Created container startup-probe-tcpsocket-succeed-demo
14s         Normal    Started                pod/startup-probe-tcpsocket-succeed-demo   Started container startup-probe-tcpsocket-succeed-demo
```
### startupProbe的实践建议
1. 对于那些启动时间比较长的Container，建议配置startupProbe；

2. startupProbe最好和livenessProbe、readiness同时配置；

3. startupProbe的类型和执行的命令，最好和livenessProbe保持一致；这样避免startupProbe执行成功，而livenessProbe失败导致Container重启，而很难去定位和分析问题；

4. startupProbe的periodSeconds*failureThreshold一定要大于Container启动的需要时长。否则，该时间段内，Container不能完成启动，将被重启，下一次又没能在该时长范围内完成启动，再次被重启，陷入死循环；


